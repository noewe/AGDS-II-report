---
title: "Digital Soil Mapping"
author: 'Noémie Wellinger'
date: "`r Sys.Date()`"
output: html_document
---

```{r libs, include = F}
source("../R/check_pkg.R")
pkgs <- c("ggplot2", "dplyr", "tidyverse", "cowplot", "pheatmap", "vcd", "gridExtra", "pROC")
check_pkg(pkgs)
```


# Simple model
To begin, a simple Random Forest model is trained for the binary categorical variable `waterlog.100`, indicating wether the soil is waterlogged at 100 cm depth. The code can be found in the analysis folder.

## Variable importance
The variable importance plot shows that the fiv mo
```{r simple_model, echo = TRUE}
# Load random forest model
rf_basic <- readRDS(here::here("data/rf_basic_waterlog100.rds"))

# Extract the variable importance and create a long tibble
vi_rf_basic <- rf_basic$variable.importance |>
  dplyr::bind_rows() |> 
  tidyr::pivot_longer(cols = dplyr::everything(), names_to = "variable")

# Plot variable importance, ordered by decreasing value
vi_rf_basic |> 
  ggplot2::ggplot(ggplot2::aes(x = reorder(variable, value), y = value)) +
  ggplot2::geom_bar(stat = "identity", fill = "grey50", width = 0.75) + 
  ggplot2::labs(
    y = "Change in OOB MSE after permutation", 
    x = "",
    title = "Variable importance based on OOB") +
  ggplot2::theme_classic() +
  ggplot2::coord_flip()
```

*Evaluate the model on the testing subset of the data. Consider appropriate metrics as described in AGDS Book Chapter 8.3. Is the data balanced in terms of observed TRUE and FALSE values? What does this imply for the interpretation of the different metrics?*

# Variable selection
As explained in the tutorial, the Boruta algorithm is used to perform a variable selection based on the variable importance (Kursa & Rudnicki, 2010).
#TODO: figure out how referencing works.

```{r}
# load the Boruta object
bor <- readRDS(here::here("data/boruta_waterlog100.rds"))
  
# obtain results: a data frame with all variables, ordered by their importance
df_bor <- Boruta::attStats(bor) |> 
  tibble::rownames_to_column() |> 
  dplyr::arrange(dplyr::desc(meanImp))

# plot the importance result  
ggplot2::ggplot(ggplot2::aes(x = reorder(rowname, meanImp), 
                             y = meanImp,
                             fill = decision), 
                data = df_bor) +
  ggplot2::geom_bar(stat = "identity", width = 0.75) + 
  ggplot2::scale_fill_manual(values = c("grey30", "tomato", "grey70")) + 
  ggplot2::labs(
    y = "Variable importance", 
    x = "",
    title = "Variable importance based on Boruta") +
  ggplot2::theme_classic() +
  ggplot2::coord_flip()

# get retained important variables
predictors_selected <- df_bor |> 
  dplyr::filter(decision == "Confirmed") |>
  dplyr::pull(rowname)

print(paste(length(predictors_selected)," of ", nrow(df_bor), " variables were judged `confirmed` by the Boruta algorithm."))
```

Let's see which ones these were compared to the original variable importance:

```{r}
df_bor_iv_basic <- df_bor |>
  rename(variable = rowname) |>
  full_join(vi_rf_basic, by='variable') |>
  rename(vi_original = value)

# plot the importance result  
ggplot2::ggplot(ggplot2::aes(x = reorder(variable, vi_original), 
                             y = vi_original,
                             fill = decision), 
                data = df_bor_iv_basic) +
  ggplot2::geom_bar(stat = "identity", width = 0.75) + 
  ggplot2::scale_fill_manual(values = c("grey30", "tomato", "grey70")) + 
  ggplot2::labs(
    y = "Original Variable importance", 
    x = "",
    title = "Original VI and Variables chosen by Boruta") +
  ggplot2::theme_classic() +
  ggplot2::coord_flip()
```

The Random Forest is the retrained using only the variables selected by the Boruta algorithm.

# Model optimization
To optimize the hyperparameters ´mtry´ and ´min.node.size´, I used grid hyperparameter tuning and a 5-fold cross-validation. The grid is passed to the train()-function via the tuneGrid argument, as shown in chapter 11 of the AGDS I book. The code is stored in the analysis folder.
The definitions of these two hyperparameters, according to AGDS I book, are:  
* mtry: The number of variables to consider to make decisions at each node, often taken as p/3 for regression, where p is the number of predictors. Default is the (rounded down) square root of the number variables, so 6 in our case
* min.node.size: The number of data points at the “bottom” of each decision tree, i.e. the leaves. Default 1 for classification.

The hyperparameter tuning turned out to be rather uneffective. I performed multiple model runs with the same tuning grid and different seed values. As can be seen on the 6 example figures below, the accuracy of the hyperparameter combination quite random for min.node.size between 1 and 20 and for mtry between 1 and 10. When expanding the ranges of the two hyperparameters, we can see that the values start to minimize after a min.node.size of roughly.

!("../manuscript/figures/hyperparam_tuning.png")

I decided to take the combination of hyperparameters that had the biggest accuracy out of all the six tuning runs. This means, that the corresponding seed value also has to be used, when calculating the final model.


# Comparison of the three models
We can now compare the simple model and the retrained model after variable selection.
Since the waterlog.100 is a binary categorical variable, we need to use metrics for classification, that indicate how many validation sites are correctly classified as either TRUE or FALSE. This is represented in a confusion matrix.
```{r}
# Load random forest model
rf_bor   <- readRDS(here::here("data/rf_bor_waterlog100.rds"))
rf_opt   <- readRDS(here::here("data/rf_opt_waterlog100.rds"))

# Load the data
df_full <- readRDS(here::here("data/df_full.rds")) |>
  mutate(waterlog.30 = as.factor(waterlog.30),
         waterlog.50 = as.factor(waterlog.50),
         waterlog.100 = as.factor(waterlog.100))

# Split dataset into training and testing sets
df_test  <- df_full |> dplyr::filter(dataset == "validation") |> tidyr::drop_na()

# Make predictions for validation sites using the 2 models 
prediction_basic <- predict(
  rf_basic,           # RF model
  data = df_test,   # Predictor data
  num.threads = parallel::detectCores() - 1
  )

prediction_bor <- predict(
  rf_bor,           # RF model
  data = df_test,   # Predictor data
  num.threads = parallel::detectCores() - 1
  )

prediction_opt <- predict(
  rf_opt,           # RF model
  data = df_test,   # Predictor data
  num.threads = parallel::detectCores() - 1
  )

# Save predictions to validation df
df_test$pred_basic <- prediction_basic$predictions
df_test$pred_bor <- prediction_bor$predictions
df_test$pred_opt <- prediction_opt$predictions
```

```{r}
# plot confusion matrix
conf_matrix_basic <- caret::confusionMatrix(data = df_test$pred_basic, reference = df_test$waterlog.100)
conf_matrix_bor <- caret::confusionMatrix(data = df_test$pred_bor, reference = df_test$waterlog.100)
conf_matrix_opt <- caret::confusionMatrix(data = df_test$pred_opt, reference = df_test$waterlog.100)
mosaicplot(conf_matrix_basic$table,
           main = "Confusion matrix simple model")
mosaicplot(conf_matrix_bor$table,
           main = "Confusion matrix Boruta model")
mosaicplot(conf_matrix_opt$table,
           main = "Confusion matrix optimized model")
conf_matrix_basic
conf_matrix_bor
conf_matrix_opt
```

```{r}
# Create a list of confusion matrices
conf_matrices <- lapply(list(
  "Basic model" = conf_matrix_basic,
  "Model with reduced predictors" = conf_matrix_bor,
  "Optimized model" = conf_matrix_opt
), as.table)

# Create mosaic plots for each confusion matrix
mosaic_plots <- lapply(conf_matrices, function(conf_matrix) {
  mosaic(conf_matrix, main = "")
})

# Arrange mosaic plots in a single grid
grid.arrange(grobs = mosaic_plots, ncol = 3)
```

# Probabilistic predictions
A new, probabilistic model was trained using the default hyperparameters, and the reduced predictor set according to the Boruta selection.
Then, the model is first tested and evaluated on the validation sites.
The Receiver Operating Characteristic (ROC) curve evaluates the performance of a binary classification model.
```{r roc, echo = T}
rf_prob <- readRDS(here::here("data/rf_prob_waterlog100.rds"))
prediction_prob <- predict(
  rf_prob,           # RF model
  data = df_test,   # Predictor data
  num.threads = parallel::detectCores() - 1
  )

# save the predictions that waterlog.100 = TRUE (1)
pred_prob <- prediction_prob$predictions 
df_test$pred_prob <- pred_prob[,2]

# Plot the probability
df_test |> 
  ggplot2::ggplot(ggplot2::aes(x = pred_prob, y = waterlog.100, color = waterlog.100)) +
  ggplot2::geom_point() +
  ggplot2::theme_classic() +
  ggplot2::labs(
    title = "Predicted probability vs. Observed soil waterlog at 100 cm",
    # subtitle = bquote(paste("Bias = ", .(bias), 
    #                         ", RMSE = ", .(rmse), 
    #                         ", R"^2, " = ", .(r2))),
    x = "Predicted probability",
    y = "Observed waterlog"
  )

# Create the ROC curve
roc_obj <- roc(df_test$waterlog.100, df_test$pred_prob)
# Plot the ROC curve
plot(roc_obj, print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1, 0.1),
grid.col=c("darkgrey", "lightgrey"), max.auc.polygon=TRUE,
auc.polygon.col="#ccffaa", print.thres=F, main = "ROC curve for soil waterlog at 100 cm")
```

Let's map the predicted probability:
```{r map_raster_prob, echo = TRUE}
raster_pred_prob <- terra::rast(here::here("data/predicted_waterlog100_prob.tif"))

ggplot2::ggplot() +
  tidyterra::geom_spatraster(data = raster_pred_prob) +
  ggplot2::scale_fill_viridis_c(
    na.value = NA,
    option = "viridis",
    name = "probability"
  ) +
  ggplot2::theme_classic() +
  ggplot2::scale_x_continuous(expand = c(0, 0)) +
  ggplot2::scale_y_continuous(expand = c(0, 0)) +
  ggplot2::labs(title = "Predicted probability of soil waterlogging (100 cm)")
```



# References
Kursa, M. B., & Rudnicki, W. R. (2010). Feature Selection with the Boruta Package. Journal of Statistical Software, 36(11), 1–13. https://doi.org/10.18637/jss.v036.i11

Robin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F., Sanchez, J.-C., & Müller, M.
(2011). pROC: an open-source package for R and S+ to analyze and compare ROC curves. BMC Bioinformatics, 12, 77.
https://doi.org/10.1186/1471-2105-12-77